% Generated by roxygen2 (4.1.0): do not edit by hand
% Please edit documentation in R/plotFeatureImportance.R
\name{plotFeatureImportance}
\alias{plotFeatureImportance}
\title{Feature Importance Plot}
\usage{
plotFeatureImportance(featureList, control = list(), ...)
}
\arguments{
\item{featureList}{[\code{list}]\cr
List of vectors of features. One list element is expected to belong to
one resampling iteration / fold.}

\item{control}{[\code{list}]\cr
A list object that stores additional configuration parameters.
Among these parameters, one can define the colors
(featimp.col_{high/medium/low}) and thresholds (featimp.perc_{high/low}),
which are used to highlight the difference between often and rarely used
features. One can also define the alignment of the axis labels
(featimp.las), the labels itself (featimp.{xlab/ylab}), the angle of the
features on the y-axis (featimp.string_angle) and the type
(featimp.pch_{active/inactive}) and color (featimp.col_inactive) of the
inactive points and lines.}

\item{...}{[any]\cr
Further arguments to be passed into the plot function.}
}
\value{
[\code{plot}].\cr
  Feature Importance Plot, indicating which feature was used during which iteration.
}
\description{
Creates a feature importance plot.
}
\examples{
# At the beginning, one needs a list of features, e.g. derived during a
# nested feature selection within mlr (see the following 8 steps):
library(mlr)
library(mlbench)
data(Glass)

# (1) Create a classification task:
classifTask = makeClassifTask(data = Glass, target = "Type")

# (2) Define the model (here, a classification tree):
lrn = makeLearner(cl = "classif.rpart")

# (3) Define the resampling strategy, which is supposed to be used within
# each inner loop of the nested feature selection:
innerResampling = makeResampleDesc("Holdout")

# (4) What kind of feature selection approach should be used? Here, we use a
# sequential backward strategy, i.e. starting with a model based on the
# entire feature set, a feature is iteratively dropped (based on a greedy)
# approach:
ctrl = makeFeatSelControlSequential(method = "sbs")

# (5) Wrap the original model (see (2)) in order to allow feature selection:
wrappedLearner = makeFeatSelWrapper(learner = lrn,
  resampling = innerResampling, control = ctrl)

# (6) Define a resampling strategy for the outer loop. This is necessary in
# order to assess whether the selected features depend on the underlying
# fold:
outerResampling = makeResampleDesc(method = "CV", iters = 3)

# (7) Perform the feature selection:
#featselResult = resample(learner = wrappedLearner, task = classifTask,
# resampling = outerResampling, models = TRUE)

# (8) Extract the features, which were selected during each iteration of the
# outer loop (i.e. during each of the 5 folds of the cross-validation):
#featureList = lapply(featselResult$models,
#  function(mod) getFeatSelResult(mod)$x)


# Now, one could inspect the features manually:
#featureList

# Alternatively, one might use visual means such as the feature
# importance plot:
#plotFeatureImportance(featureList)
}

